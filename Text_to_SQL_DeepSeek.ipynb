{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTDhSM4/hy/btfieXkKkX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c677edf10a245cc8759b4493ee62779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff54b5ce338145f0824d2fc3eeac221a",
              "IPY_MODEL_dee3e37979914447b7ebad9647e4d695",
              "IPY_MODEL_c9f7b40241864e9eb97d7e4b4eded65f"
            ],
            "layout": "IPY_MODEL_4215ae48efef4bc58eed0341d68b5f28"
          }
        },
        "ff54b5ce338145f0824d2fc3eeac221a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94fc9505c4dc40b0aa344949c970849b",
            "placeholder": "​",
            "style": "IPY_MODEL_7301b6976509417ca902f63129ed72e8",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "dee3e37979914447b7ebad9647e4d695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_778cd0e7b12140b7bc45ec88f3437721",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe6779da124947239e81fffc9b9f0416",
            "value": 0
          }
        },
        "c9f7b40241864e9eb97d7e4b4eded65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82301c1157c04fb5aa4be8b8e6fc79e3",
            "placeholder": "​",
            "style": "IPY_MODEL_019f632880554f749c1002300e20e3ad",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "4215ae48efef4bc58eed0341d68b5f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94fc9505c4dc40b0aa344949c970849b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7301b6976509417ca902f63129ed72e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "778cd0e7b12140b7bc45ec88f3437721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6779da124947239e81fffc9b9f0416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82301c1157c04fb5aa4be8b8e6fc79e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "019f632880554f749c1002300e20e3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parshvak26/Text-to-SQL/blob/main/Text_to_SQL_DeepSeek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sof50ANkoizG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class DeepseekTxt2SQLPipeline:\n",
        "    def __init__(self, model_folder: str,\n",
        "                 system_message: str = None,\n",
        "                 device: str = \"cuda\",\n",
        "                 torch_dtype=\"auto\"):\n",
        "        \"\"\"\n",
        "        Initializes the Deepseek pipeline specialized for text-to-SQL tasks.\n",
        "\n",
        "        Args:\n",
        "            model_folder (str): Path to the model folder.\n",
        "            system_message (str, optional): Custom system instruction message.\n",
        "                Defaults to an SQL assistant instruction.\n",
        "            device (str): The device to use (\"cuda\" by default).\n",
        "            torch_dtype: Data type for model weights.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "\n",
        "        # Clear memory before loading the model, conditionally for CUDA\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()  # Clear unused cached memory\n",
        "            torch.cuda.ipc_collect()  # Collect unused memory\n",
        "        else:\n",
        "             gc.collect() # Collect garbage for CPU\n",
        "\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_folder,\n",
        "            torch_dtype=torch_dtype,\n",
        "            device_map=self.device\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
        "\n",
        "        # Default system message tailored for SQL query generation\n",
        "        if system_message is None:\n",
        "            self.system_message = (\n",
        "                \"You are Qwen, an expert SQL assistant. \"\n",
        "                \"You help users convert natural language descriptions into efficient and accurate SQL queries.\"\n",
        "            )\n",
        "        else:\n",
        "            self.system_message = system_message\n",
        "\n",
        "    def apply_chat_template(self, messages, tokenize=False, add_generation_prompt=True):\n",
        "        \"\"\"\n",
        "        Applies the chat template using the tokenizer's built-in method.\n",
        "        Assumes that the tokenizer supports apply_chat_template.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=tokenize,\n",
        "            add_generation_prompt=add_generation_prompt\n",
        "        )\n",
        "\n",
        "    def generate_response(self, prompt: str, max_new_tokens: int = 2000) -> str:\n",
        "        \"\"\"\n",
        "        Generates a response using the Deepseek model.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The input prompt.\n",
        "            max_new_tokens (int): Maximum number of tokens to generate.\n",
        "\n",
        "        Returns:\n",
        "            str: The model's generated response.\n",
        "        \"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_message},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        # Build the full text prompt using the chat template\n",
        "        text = self.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Tokenize the input text and move it to the proper device\n",
        "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Generate tokens from the model\n",
        "        generated_ids = self.model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "\n",
        "        # Remove the input tokens from the generated output tokens\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):]\n",
        "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        return response\n",
        "\n",
        "    def generate_query(self, prompt: str, max_new_tokens: int = 2000) -> str:\n",
        "        \"\"\"\n",
        "        Generates a SQL query based on a natural language description.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): A description of the SQL query requirements.\n",
        "            max_new_tokens (int): Maximum number of tokens for generation.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated SQL query.\n",
        "        \"\"\"\n",
        "        sql_prompt = (\n",
        "            f\"Convert the following natural language description into a SQL query:\\n\\n\"\n",
        "            f\"{prompt}\\n\\nSQL Query:\"\n",
        "        )\n",
        "        return self.generate_response(sql_prompt, max_new_tokens)\n",
        "\n",
        "    def optimize_query(self, query: str,\n",
        "                       optimization_instructions: str = \"Optimize the following SQL query for better performance and readability.\",\n",
        "                       max_new_tokens: int = 2000) -> str:\n",
        "        \"\"\"\n",
        "        Optimizes an existing SQL query.\n",
        "\n",
        "        Args:\n",
        "            query (str): The SQL query that needs optimization.\n",
        "            optimization_instructions (str): Instructions for optimization.\n",
        "            max_new_tokens (int): Maximum number of tokens for generation.\n",
        "\n",
        "        Returns:\n",
        "            str: The optimized SQL query.\n",
        "        \"\"\"\n",
        "        prompt = f\"{optimization_instructions}\\n\\n{query}\\n\\nOptimized SQL Query:\"\n",
        "        return self.generate_response(prompt, max_new_tokens)\n",
        "\n",
        "    def generate_query_with_explanation(self, prompt: str, max_new_tokens: int = 2000) -> str:\n",
        "        \"\"\"\n",
        "        Generates a SQL query and provides detailed explanations.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): A description of the SQL query requirements.\n",
        "            max_new_tokens (int): Maximum number of tokens for generation.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated SQL query along with detailed explanations.\n",
        "        \"\"\"\n",
        "        full_prompt = (\n",
        "            f\"Please generate a SQL query based on the following requirements: {prompt}\\n\\n\"\n",
        "            \"After generating the SQL query, provide a detailed explanation of each part of the query.\"\n",
        "        )\n",
        "        return self.generate_response(full_prompt, max_new_tokens)\n",
        "\n",
        "    def generate_diverse_drafts(self, prompt: str, max_new_tokens: int = 2000) -> dict:\n",
        "        \"\"\"\n",
        "        Generates diverse SQL drafts in different dialects (Standard SQL, SQL Server, MySQL)\n",
        "        and returns responses in three formats: text, markdown, and json.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The natural language description for the SQL query.\n",
        "            max_new_tokens (int): Maximum number of tokens for generation.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary with keys 'text', 'markdown', and 'json' containing the responses.\n",
        "        \"\"\"\n",
        "        # Create tailored prompts for each SQL dialect\n",
        "        standard_prompt = f\"Convert the following description into a Standard SQL query:\\n{prompt}\\nSQL Query:\"\n",
        "        sqlserver_prompt = f\"Convert the following description into a SQL Server query:\\n{prompt}\\nSQL Query:\"\n",
        "        mysql_prompt = f\"Convert the following description into a MySQL query:\\n{prompt}\\nSQL Query:\"\n",
        "\n",
        "        # Generate responses using the generate_response method\n",
        "        standard_sql = self.generate_response(standard_prompt, max_new_tokens)\n",
        "        sqlserver_sql = self.generate_response(sqlserver_prompt, max_new_tokens)\n",
        "        mysql_sql = self.generate_response(mysql_prompt, max_new_tokens)\n",
        "\n",
        "        # Compose plain text response\n",
        "        text_response = (\n",
        "            \"Draft 1 (Standard SQL):\\n\" + standard_sql + \"\\n\\n\" +\n",
        "            \"Draft 2 (SQL Server):\\n\" + sqlserver_sql + \"\\n\\n\" +\n",
        "            \"Draft 3 (MySQL):\\n\" + mysql_sql\n",
        "        )\n",
        "\n",
        "        # Compose markdown response\n",
        "        markdown_response = (\n",
        "            \"#### Draft 1: Standard SQL\\n\"\n",
        "            \"```sql\\n\" + standard_sql + \"\\n```\\n\\n\"\n",
        "            \"#### Draft 2: SQL Server\\n\"\n",
        "            \"```sql\\n\" + sqlserver_sql + \"\\n```\\n\\n\"\n",
        "            \"#### Draft 3: MySQL\\n\"\n",
        "            \"```sql\\n\" + mysql_sql + \"\\n```\\n\"\n",
        "        )\n",
        "\n",
        "        # Compose JSON response as a dictionary\n",
        "        json_response = {\n",
        "            \"drafts\": [\n",
        "                {\"dialect\": \"Standard SQL\", \"query\": standard_sql},\n",
        "                {\"dialect\": \"SQL Server\", \"query\": sqlserver_sql},\n",
        "                {\"dialect\": \"MySQL\", \"query\": mysql_sql}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Return responses in all formats\n",
        "        return {\n",
        "            \"text\": text_response,\n",
        "            \"markdown\": markdown_response,\n",
        "            \"json\": json_response\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.model_download(\"deepseek-ai/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b\")\n",
        "\n",
        "print(\"Path to model files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-6GJwz2tpRIU",
        "outputId": "f4325875-1d97-4f0f-b620-a852081d78b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to model files: /root/.cache/kagglehub/models/deepseek-ai/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gevf8sHpDExC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your model folder\n",
        "model_folder = \"/root/.cache/kagglehub/models/deepseek-ai/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b/2\"\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = DeepseekTxt2SQLPipeline(model_folder=model_folder, device=\"cpu\")\n",
        "\n",
        "# Example natural language prompt for SQL query generation\n",
        "prompt = \"Retrieve all orders placed in the last 30 days from the orders table.\"\n",
        "\n",
        "# Generate diverse SQL drafts in multiple formats (plain text, markdown, and JSON)\n",
        "diverse_drafts = pipeline.generate_diverse_drafts(prompt=prompt)\n",
        "\n",
        "# Print the diverse outputs\n",
        "\n",
        "# Plain Text Output\n",
        "print(\"Plain Text Output:\\n\")\n",
        "print(diverse_drafts['text'])\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Markdown Output\n",
        "print(\"Markdown Output:\\n\")\n",
        "print(diverse_drafts['markdown'])\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# JSON Output\n",
        "print(\"JSON Output:\\n\")\n",
        "print(diverse_drafts['json'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4c677edf10a245cc8759b4493ee62779",
            "ff54b5ce338145f0824d2fc3eeac221a",
            "dee3e37979914447b7ebad9647e4d695",
            "c9f7b40241864e9eb97d7e4b4eded65f",
            "4215ae48efef4bc58eed0341d68b5f28",
            "94fc9505c4dc40b0aa344949c970849b",
            "7301b6976509417ca902f63129ed72e8",
            "778cd0e7b12140b7bc45ec88f3437721",
            "fe6779da124947239e81fffc9b9f0416",
            "82301c1157c04fb5aa4be8b8e6fc79e3",
            "019f632880554f749c1002300e20e3ad"
          ]
        },
        "id": "CqhQj201q4yC",
        "outputId": "1575d686-46f5-4f46-a70a-549779e4aac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c677edf10a245cc8759b4493ee62779"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFXVXPCYDGXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install dependencies (quietly)\n",
        "!pip -q install --upgrade \"transformers>=4.44.0\" \"accelerate>=0.34.0\" bitsandbytes sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO1y2pXcEnWk",
        "outputId": "13c672eb-2dca-437c-ef0a-420d6aaddfa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    }
  ]
}